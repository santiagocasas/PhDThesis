\chapter{Statistics in Cosmology} % Main chapter title

\label{Statis} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

\section{The relation between Gaussianity, linearity and homogeneity}

\subsection{Gaussian random fields}

In cosmology, we are interested in studying observables which \revtext{are} the product of a large number of random processes.
The best examples are the fluctuations of temperature in the CMB $\delta T / T$ or the fluctuations of the matter density in the Universe, 
usually called $\delta$, which originate from complicated interactions between different matter species, gravity and other fundamental forces.
Using the Central Limit Theorem \cite{(cite...)}, we can show that the sum of a large number of random variables, that originate from independent random processes, will yield a variable that it is almost exactly Gaussian distributed. The Gaussian probability distribution $p(x)$ for a random variable $x$, with mean $\mu$ and variance $\sigma$ is given by
\begin{equation}
p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left( \frac{(x-\mu)^2}{2\sigma^2} \right)
\end{equation}
As we sill see, \revtext{Gaussian} distributed variables will be very important in observations and analysis of cosmological observables.
In the case of structure formation, $\delta$ is a random field that describes inhomogeneities, however, due to the 
assumption that it originates from independent random processes in a homogeneous and isotropic universe,
its statistical properties must be homogeneous and isotropic too \cite{(cite...Peacock, Bjoern, ?)}.
That means that the probability distribution $p(\delta(\vec{x}))$ must be invariant under translations and rotations of space.
The expectation value of $\langle \delta^n \rangle$ is formally speaking defined by an ensemble average, meaning that
we should actually observe this process in many random realizations of the Universe. Since this is not possible,
in cosmology we postulate the ergodicity principle \cite{(cite Adler...)}, which states that for sufficient large volumes
the ensemble average is equal to the volume average and therefore, we can write:
\begin{equation}
\langle \delta^n \rangle = \frac{1}{V} \int_V \mathrm{d}^3 x \, \delta^n (\vec{x}) p(\delta (\vec{x}))
\end{equation} 
The importance of this postulate in practical terms is that we can use our formal statistical tools 
to compute theoretically the expectation values
of observables \comm{the sentence from here on is not clear, what matches what? why do you need this paragraph?} and on the observational side, we just need to match this by observing over large volumes in the sky.

\subsection{The two-point correlation function}

The advantage of working with fluctuation variables in cosmology is that their mean is zero, i.e. $\langle \delta \rangle = 0$ \comm{mmm I am not sure what do you mean here; in general please only say things that you really need}
and that they remain small ($\delta<<1$) for most of their evolution time.
Therefore what makes sense to study \comm{too colloquial, please use formal language} is not its mean, but its variance, $\langle \delta^2 \rangle$, or if 
we are measuring the fluctuations at two separated points in the universe ($\vec{x}$ and $\vec{y}$),
then it makes sense to study their two-point correlation function, defined as:
\begin{equation} \label{eq:correlationfunct}
\xi(\vec{x} ,\vec{y}) = \langle \delta (\vec{x})  \delta (\vec{y}) \rangle
\end{equation}
As we have seen before, $\delta$ is statistically homogeneous and isotropic. Therefore, due to homogeneity, the correlation
function can only depend on separation $\vec r = \vec{x}_2 - \vec{x}_1 $ \comm{x1, x2 not defined, we had x and y and r} and due to invariance under 
rotations it can only depend on the magnitude of the separation, denoted as $r$.
In simple words---for the case of large scale structure---, $\xi(r)$ is telling us how probable it is to find
a galaxy a distance $r$ away from a given galaxy at position $\vec x$.
We are talking here about \comm{too informal} ideal measurements for a linear and Gaussian observable, but in the later chapters, when we discuss
about non-linearities and observational effects (for example redshift space distortions), we will see that the situation is in practice not so simple and 
the observable $\delta$, will not be specified only by its 
two-point correlation function and will not be statistically isotropic, leading to different correlation
functions parallel and perpendicular to the line of sight \cite{(cite some BAO stuff)}.

\subsection{The data covariance matrix}

Let us introduce at this point the \emph{data} covariance matrix, not to be confused with the \emph{parameter} covariance matrix, which will be introduced later. \comm{never use sentences like: since this is a concept that causes a lot of misunderstanding in the literature; just explain what you think it's good, there is no need to criticize others}
If we are measuring the fluctuation $\delta$ at three points in space,  $\vec{x}, \vec{y}$ and $\vec{z}$,
then their joint probability density will still be Gaussian:
\begin{equation}\label{eq:multivariateGaussian}
p( \delta (\vec{x}),\delta (\vec{y}),\delta (\vec{z})) = 
\frac{1}{\sqrt{(2 \pi)^2 \det \mathbf{C}}}\exp\left( -\frac{1}{2} \colvec{3}{\delta(\vec{x})}{\delta(\vec{y})}{\delta(\vec{z})}^T 
\mathbf{C}^{-1} \colvec{3}{\delta(\vec{x})}{\delta(\vec{y})}{\delta(\vec{z})}
 \right)
\end{equation}
where now instead of a Gaussian dispersion $\sigma$, we have a Gaussian \emph{data} covariance matrix $\mathbf{C}$, given by:
\begin{equation}\label{eq:dataCovariance-abstract}
C_ij = 
\begin{pmatrix}
\langle \delta^2 (\vec{x}) \rangle 
                & \langle \delta (\vec{x}) \delta (\vec{y})\rangle 
                        & \langle \delta (\vec{x})  \delta (\vec{z})\rangle  \\
\langle \delta (\vec{x})  \delta (\vec{y})\rangle 
                & \langle \delta^2 (\vec{y}) \rangle 
                        & \langle \delta (\vec{y})  \delta (\vec{z})\rangle  \\
\langle \delta (\vec{x})  \delta (\vec{z})\rangle 
                & \langle \delta (\vec{y}) \delta (\vec{z})\rangle 
                        & \langle \delta^2 (\vec{z})  \rangle  \\
\end{pmatrix}
\end{equation}
Notice that the covariance matrix is symmetric and positive definite. This is the data covariance matrix for three measurements
in real space. If you would measure the density fluctuations at 1000 different points in space, the covariance matrix would 
have 1 million elements. Therefore, it is very important not only for theoretical, but for practical reasons of data analysis, that the covariance matrix is as simple and symmetric as possible.

Luckily, for Gaussian random fields their statistical properties can be defined entirely by the their two-point correlation function,
where the following recursion relations (obtained by Isserli's or Wick's theorem \cite{(cite Wick or something)}) are valid:
\begin{align}
\langle \delta^{2n}\rangle = (2n-1)!!\langle \delta \rangle \, & ,\quad \langle \delta^{2n+1}\rangle =0
\end{align}
\comm{Is this at a fixed position in space? Then isn't it just one of the 1 million elements in the matrix? then why is this more convenient?} This is of course expected, 
since we have defined in Eqn. \ref{eq:multivariateGaussian} the covariance matrix $\mathbf{C}$ as the only statistical quantity needed to specify
fully the distribution of the multivariate  Gaussian random field.

\subsection{The power spectrum}

Since we want observations of $\delta$ to be independent of each other, we need that the equations governing the spatial 
distribution of the density fluctuations are linear, otherwise non-linear terms can induce correlations between 
values at different positions \comm{Reformulate: we consider linear perturbations which have the advantage that they can be treated separately for each k; there is no fundamental reason why we 'want' perturbations to be independent, it's just more complicated}. We can see this more clearly if we work in Fourier space.
For linear equations, transforming into Fourier space (going from position $\vec x$ to wavevectors $\vec k$) is quite useful, 
since space derivatives become simply \comm{remove adjectives like 'simply' 'just' 'obviously' 'easy' 'straightforward' and similar} factors of $\vec k$ and the Fourier space function $\delta(\vec k)$
evolves in time, independently for each $k$-mode, but only if the field is statistically homogeneous.

The Fourier transform of the density field, denoted $\delta(\vec k)$ is defined as:
\begin{equation}
\delta(\vec k) = \int \mathrm{d}^3 x \delta(\vec x) \exp(-i \vec{k} \cdot \vec{x})
\end{equation}
and its inverse is
\begin{equation}
\delta(\vec x) = \int \frac{\mathrm{d}^3 k}{(2\pi^3)} \delta(\vec k) \exp(i \vec{k} \cdot \vec{x}) \quad .
\end{equation}
Since now $\delta(\vec k)$ can be complex, we can define the variance between two $k$-modes as:
\begin{equation}
\langle \delta (\vec{k}_1)  \delta^{*} (\vec{k}_2) \rangle = 
\int \mathrm{d}^3 x \, \mathrm{d}^3 y \langle \delta (\vec{x})  \delta (\vec{y}) \rangle \exp(-i \vec{k} \cdot (\vec{x}-\vec{y}) )
(2\pi)^3 \delta_{D}(\vec{k}_{1}-\vec{k}_{2})  \quad .
\end{equation}
Using the definition of the correlation function \ref{eq:correlationfunct} and the fact that the Gaussian random field $\delta$
is homogeneous we can define the power spectrum $P(\vec k)$ as:
\begin{equation}\label{eq:powerspectrum}
P(\vec k) = \int \mathrm{d}^3r \, \xi(\vec r) \exp(-i \vec{k} \cdot \vec{r})
\end{equation}
The relation between the expected value of $\langle \delta (\vec{k}_1)  \delta^{*} (\vec{k}_2) \rangle$ and the power spectrum $P(\vec k)$ can be obtained by a similar straightforward calculation, \cite{(cite Luca, Dodelson, ??)} where taking into account
that the $\delta$ field is real-valued, one can obtain:
\begin{equation}
\langle \delta (\vec{k}_1)  \delta (\vec{k}_2) \rangle = (2\pi)^3  P(k) \delta_{D}(\vec{k}_{1}-\vec{k}_{2}) \quad .
\end{equation}
The Dirac delta $\delta_D$ in the previous equation, shows us directly what we had expected from a random Gaussian and homogeneous field;
the Fourier modes $\vec{k}_1$ and $\vec{k}_2$ for the density fluctuations are mutually uncorrelated.
In this case \comm{you mean: at the linear level}, the data covariance matrix $\mathbf{C}$, defined in Eqn. \ref{eq:dataCovariance-abstract},
would be \comm{would be -> is } perfectly diagonal in Fourier space, making an analysis of the observations much more tractable.

However, this is only true if the equations governing $\delta$ are linear; otherwise, non-linear terms in real space,
would introduce convolutions in Fourier space, giving rise to non-vanishing correlations $\langle \delta (\vec{k}_1)  \delta (\vec{k}_2) \rangle$. This will be the subject of study of the non-linear power spectrum, which will be treated with more depth in chapter \ref{chap:nonlinear}.

\subsection{Final remarks on linearity, Gaussianity and homogeneity}

We have seen in the previous sections, that the concepts of linearity, Gaussianity and homogeneity
are strongly related in Cosmology.
The fact that observables like the matter density fluctuations originate from a large number 
of independent random processes, leads---thanks to the Central Limit Theorem---to a Gaussian random field.
Then homogeneity and isotropy of the Universe and linear equations (in space) governing those processes, ensure that the statistical properties of the Gaussian random field
are also homogeneous and isotropic and that we can use the ergodic theorem to compute ensemble averages.
The Gaussianity of the field (and its vanishing average value) allows us to describe the field, solely with its two
point correlation function, which directly connects us to the power spectrum and then to the fact that in Fourier space, 
pairs of wavemodes are mutually uncorrelated.
\begin{equation}
\textrm{Gaussianity}\Leftrightarrow\textrm{Linearity}\Leftrightarrow\textrm{Homogeneity}
\end{equation}

In contrast, non-linear evolution of the density field, would yield correlations among different $k$-modes, 
leading to a non-Gaussian probability distribution and the loss of homogeneity (or the appearance of $k$-dependent growth).
This is what happens at later stages of structure formation, 
where fluctuations are limited to $-1 < \delta$ in voids, 
but can grow to very high values $\delta \gg 1$ inside the cosmic web structure.
This in turn will make the data covariance matrix $\mathbf{C}$ in Fourier space not diagonal anymore and 
will \revtext{difficult the analysis of cosmological observations.}
Therefore, the computation
of these non-linear evolution equations 
and the analysis of higher order correlations is one of the great challenges that research in cosmology 
will have to face in the next decade.


\section{Likelihood and the Bayesian approach}

The subject of Bayesian statistics is a very complex, but very important topic in modern
statistics, data science and the physical sciences \cite{cite Gregory and others}
and it is outside of the scope of this work to try to even review its more fundamental properties.
We will just comment on the simple but very powerful concept of the likelihood function and how the Bayesian
approach to statistical analysis is best suited in a field like cosmology.

If an experiment yields a vector of random variables $x^i$, 
we have to
try to build a theoretical model, which depends on some free parameters $\theta^i$ 
that can give us the probability distribution $p(x^i | \theta^i)$ to find the variable $x^i$ 
inside an interval $\Delta x$ for a specific measurement.
Then what physicists are interested in, is in constraining the model parameters $\theta$ and try to find
the set of parameters (or the model) that best explains the data.

There are two possibilities of analyzing the results of that experiment, the \emph{frequentist}
and the \emph{Bayesian} approach.

In the \emph{frequentist} approach, the scientist would repeat many times the experiment and even change
the settings and the theoretical parameters $\theta$ in the lab, in order to find the distribution
of $x^i$'s ($p(x^i | \theta^i)$) and from there, infer the distribution of the parameters $\theta$. So this scientist would
be able to say (as it is usually done in particle physics) that a there is a 97\% probability that her 
data is distributed according to the model she has found as best fit (where one parameter is for example the Higgs mass).

In the \emph{Bayesian} approach, the reasoning is reversed. The scientist would look for the probability
of having a certain model (with model parameters) given the data that has already been taken ($p(\theta^i | x^i)$). This is more suitable
for cosmology, since we cannot repeat many times the same experiment and we cannot change
the parameters of the Universe.

How do we then "invert" $P(D | T)$ ---the probability of the data given a model--- to obtain $P(T | D)$? The solution is given by 
Bayes' theorem \cite{Bayes, Dodelson, Luca, many}:

\beeq$ \label{eq:BayesTheorem}
P(T | D) = \frac{P(D | T) P(T)}{P(D)}$
$P(T | D)$ is usually called the posterior probability (of having a theory $T$ given the data $D$), 
$P(T)$ is the prior probability on the theory (which quantifies our previous knowledge and prejudices)
and $P(D)$ is the evidence (the probability of the data), which for all purposes we can ignore, since it
represents just a normalization factor.
Also here, a discussion on the significance and the effects of the prior is out of our scope. \cite{cite Amendola, Dodelson, Gregory, reviews}. However, in our case it will be an efficient way of combining results from previous
experiments into our predictions for future experiments. The prior can be as simple as the theoretical expectation
that a parameter cannot be zero (for example the matter density of the Universe) or it can be the entire
probability distribution of a previous experiment.
After this point we will roughly call the posterior $P(T | D)$ by its more common name: the likelihood function
$L(\theta | x)$.

From the likelihood we can obtain some useful quantities:
\begin{itemize}
	\item The maximum likelihood estimators $\hat{\theta}_{i}$,
	which are found by solving \\ $\partial L(\theta_i)/\partial\theta_i = 0$.
	This would amount to the 'best fit' parameters, but they
	are in general different tp the frequentist ones, if we have used a prior.
	\item The confidence regions for the parameters, denoted $R(\alpha)$, for which
	the normalized likelihood integrated in that region has a specific value:\\ 
	\beeqp$\label{eq:likeli-confidregions}
	\int_{R(\alpha)}  L(\theta_i) \mathrm{d}^n \theta = \alpha$
	If $\alpha=0.683,0.954,0.997$, this regions are called 1, 2 and 3$\sigma$ regions respectively.
	\item Marginalization over a parameter. If the likelihood depends on 
	three parameters $\theta^1, \theta^2, \theta^3$, but we are not interested in $\theta^3$, because it is a nuisance parameter
	or we have no clear idea how to relate it to the theory, we can marginalize over it by integrating 
	it out: $L(\theta_1, \theta_2) = \int L(\theta^1, \theta^2, \theta^3) \,\mathrm{d}\theta^3 $
	
\end{itemize}

Despite the great usefulness of the likelihood, its evaluation represents a challenge both numerically
and computationally, since it is a complicated multi-
dimensional function.
Evaluating the likelihood for a relatively coarse grid of points in parameter space, becomes unfeasible for 
more than 7 or 8 parameters. So, techniques like Markov Chain Monte Carlo (MCMC) \cite{cite some basic MCMC literature}
 have become more and more sophisticated. In this techniques, the basic idea is to explore the parameter
 space using a random walk, which is itself guided by the steepness or flatness of the likelihood at that point.
 In this way there are much less evaluations in places where the likelihood is smooth and many evaluations
 where the likelihood changes very rapidly. Two very well known
 codes in the cosmological community are \textsc{MontePython} and \textsc{CosmoMC} \cite{cite Cosmomc and Montepython},
 which are used to evaluate the likelihood of many large and modern experiments like \textit{Planck} \cite{cite Planck}.
 
Therefore many approximations can be made in order to estimate the likelihood or at least 
to find its maximum. 
For a Gaussian distributed data vector $\bm{x}$ with $n$ components, 
with mean $\bm{\mu}$ and covariance matrix $\bm{C}$ of dimension $n\times n$ we can 
write the likelihood as $L = \exp(-\chi^2/2)$, which results in: 

\beeq$ \label{eq:moreGeneral-Gaussian-Likelihood}
L=\frac{1}{(2\pi)^{n/2} \det (\bm{C}) } \exp \left[-\frac{1}{2} 
(\bm{x}-\bm{\mu})^T \bm{C}^{-1} (\bm{x}-\bm{\mu}) \right]
$
and we can define the matrix of data points as $\bm{D} = (\bm{x}-\bm{\mu})^T  (\bm{x}-\bm{\mu}) $
and in full generality the covariance matrix $\bm C$ is just given by the expected value of $\bm D$, namely 
$\bm C = \langle \bm D \rangle $.

We will see in the next section that a very useful approximation is to say that the likelihood is Gaussian
not only in the data, but also in the parameters. In this case, a useful quantity will
be the log-likelihood $\Li \equiv \ln L$, since we can get rid of the exponential function.
For (approximately) Gaussian distributed parameters, the parameter covariance matrix will be of extremely
importance to analyze and forecast the results of large future experiments.

%\subsection{Methodology of Fisher forecasts}
%\begin{itemize}
%\item Derivatives of cosmological functions
%\item Integration or summation in $k$ or $\ell$ space
%\item Alcock-Paczynski effect
%\item Redshift binning 
%\item Shape, redshift and nuisance parameters
%\item Marginalization
%\end{itemize}

\section{Fisher Matrix formalism}
%\begin{itemize}
%\item Approximation of the Gaussian likelihood at the minimum
%\item Ellipses and confidence contours
%\item Marginalization and maximization
%\end{itemize}
We have seen in the previous section, how the log-likelihood function $\Li$ is a powerful
tool to estimate the probability distribution of model parameters, given some previously measured data.
But what if we don't have data avilable yet? For example when we want to know 
with which accuracy a certain future experiment is going to be able to constrain certain parameters of
a model. 
This is where the Fisher formalism \cite{(cite Fisher, Tegmark, etc.)} enters.
The Fisher matrix is defined as the expectation value of the \emph{curvature} of the log-likelihood:
\begin{equation}\label{eq:Fishermatrix-definition}
F_{ij} = -\left< \frac{\partial^2 \Li(\bm{\theta})}{\partial \theta_i \theta_j} \right>
\end{equation}

If we assume that the errors on the data measurements are Gaussianly distributed (where then we can write the likelihood as 
$L = e^{-\chi^2 / 2}$) then we can also think of the Fisher matrix as a Taylor expansion of the log-likelihood around its maximum
$\hat \theta$, or equivalently where the $\chi^2$ is a minimum:
\begin{equation}
\Li(\theta) = \Li(\hat \theta) + (\theta - \hat \theta)\parder{\Li}{\theta} + 
\frac{1}{2}(\theta - \hat \theta)^2 \pardersq{\Li}{\theta}
\end{equation}
The linear derivative term in the above equation vanishes, since we are expanding around the maximum and we are considering here 
for simplicity just one parameter $\theta$; but the generalization to more parameters is straightforward.
The fact that the Fisher matrix is related to the curvature of the log-likelihood around its maximum, tells us that the Fisher matrix 
is an indication of how fast the likelihood changes around the peak \cite{cite Dodelson...}. 
If the curvature is high, then the likelihood changes
fast and the experiment is very constraining, allowing for just small changes of the parameters, before the likelihood becomes too small.
If the curvature is low, then the likelihood is very flat and the experiment is not very constraining in parameter 
space \footnote{The relation between information matrices and geometry is not only qualitative, it is a 
formal field of study called Information Geometry \cite{cite Springer book}}.

\subsection{The Fisher matrix for a galaxy power spectrum}

In order to understand better the following sections where we will talk about the 
Fisher matrix of the galaxy clustering and weak lensing observables, let us specify the Fisher
matrix defined in Eqn. \ref{eq:Fishermatrix-definition} in a more concrete way.
In this section we follow closely the argumentations of \cite{cite Dodelson} and \cite{cite Amendola}.

We are interested in finding the distribution of matter in the Universe, and as we have seen before,
for large sufficient scales, the overdensity of matter at a specific scale $k$ is given by $\delta(k)$.
For a galaxy survey, covering a volume $V(z+ \Delta z)$ in three-dimensional space and providing
$m$ Fourier modes $k_i$ in a redshift bin $\Delta z$, 
we can compute (the full details of the calculation can be found in \cite{cite Dodelson})
that the data covariance matrix between mode $k_i$ and mode $k_j$ is given by:
\beeq$\label{eq:dataCovariancePk}
V \langle\delta_{k_i} \delta^*_{k_j} \rangle = C_{k_{i} k_{j}} = \frac{\delta_{ij}}{V} \left( P(k_i, z) + \frac{1}{\bar{n}(z+\Delta z)} \right)
$
The data covariance $C_{k_{i} k_{j}}$ is always a sum of the signal covariance and the noise covariance, which 
in this case is simply given by the Poisson noise of a discrete random distribution, where $\bar n$ is the 
average number density of galaxies. Again, let us emphasize that we are at relatively large scales, where then
we can take our data covariance matrix as being diagonal.
If we now assume that the galaxy distribution is well approximated by a Gaussian distribution (and we 
know it has to be since we are at linear and homogeneous scales), then we can
write the likelihood function as:
\beeq$\label{eq:likelihood-Pofk}
L=\frac{1}{(2\pi)^{m/2} \det (C_{k_{i} k_{j}}) } \exp \left[-\frac{1}{2}\sum_{i}^{m} 
\frac{\delta^2_{k_i}}{C_{k_{i} k_{j}}} \right] $
Then the log-likelihood can be written in the following way
\beeq$\label{eq:loglik-Pk}
\Li = -\ln L = \frac{m}{2} \ln(2\pi) + \sum_{i}^{m}\ln(C_{k_{i} k_{i}}) + \sum_{i}^{m}\frac{\delta^2_{k_i}}{2 C_{k_{i} k_{i}}}
$
where we have used the matrix identity: $\ln \det C = \mathrm{tr} \ln C$, which in this case is anyway trivial since $C$ is a diagonal matrix.
Now, using definition \ref{eq:Fishermatrix-definition}, we can evaluate the expectation value of the 
curvature of the log-likelihood (in model-parameter space $\theta_{\alpha}$):
\beeq$\label{eq:Fisher-fromloglik}
F_{\alpha \beta} = \sum_{i}^{m} \left[ \pardermix{\ln(C_{k_{i} k_{i}})}{\theta_{\alpha}}{\theta_{\beta}} + 
\left\langle \delta^2_{k_i} \right\rangle \paralonemix{\theta_{\alpha}}{\theta_{\beta}}\left(\frac{1}{2  C_{k_{i} k_{i}}} \right) \right] 
$
The expectation value operator only affects the data $\delta^2_{k_i}$, because the data covariance matrix $C$ is 
already itself formed by expectation values. On the other hand, the data $\delta^2_{k_i}$ cannot be affected by
derivations with respect to the model parameters, because pure measurements should be by definition model independent.
Then substituting $\left\langle \delta^2_{k_i} \right\rangle = C_{k_{i} k_{i}}$
and doing some algebra, we find:
\beeqp$ \label{eq:Fisher-fromloglik-second}
F_{\alpha \beta} = \sum_{i}^{m} \left[-
 \frac{1}{(C_{k_{i} k_{i}})^2}\parder{C_{k_{i} k_{i}}}{\theta_{\alpha}}\parder{C_{k_{i} k_{i}}}{\theta_{\beta}}
+ \frac{3}{(C_{k_{i} k_{i}})^2}\parder{C_{k_{i} k_{i}}}{\theta_{\alpha}}\parder{C_{k_{i} k_{i}}}{\theta_{\beta}} \right] 
$
Now we can use the data covariance of Eqn. \ref{eq:dataCovariancePk} to write the Fisher matrix in terms of the matter power
spectrum:
\beeqc$ \label{eq:Fisher-fromloglik-inPk}
F_{\alpha \beta} = \frac{1}{2}\sum_{i}^{m} 
 \parder{\ln P(k_i)}{\theta_{\alpha}}\parder{\ln P(k_i)}{\theta_{\beta}}
\left(  \frac{\bar{n} P(k_i)}{1+\bar{n} P(k_i)}  \right)^2
$
where we have assumed that the noise does not depend on the cosmological parameters and we have dropped
the redshift dependence for simplicity. We must remember that for a tomographic redshift survey,
this Fisher matrix has to be computed at each redshift bin.

Having done this example explicitly we can also write down the more general expression for Gaussian likelihoods, such as the one
specified in Eqn.\ref{eq:moreGeneral-Gaussian-Likelihood}, in which
the likelihood contains covariance matrices $\bm C$ which are not diagonal and the average
of the data measurements $\bm \mu$ is not zero. (see \cite{cite Luca, Dodelson}
among other works, for more details). In this case the Fisher matrix can be written as:

\beeq$ \label{eq:Fisher-MatrixForm-GaussianGeneral}
F_{\alpha \beta} = \frac{1}{2}\mathrm{Tr}\left[\bm{C}^{-1}\bm{C}_{,\alpha}\bm{C}^{-1}\bm{C}_{,\beta}
+ \bm{C}^{-1}\langle \bm{D}_{,\alpha \beta} \rangle  
\right]
$
This last term, the expected value of the derivatives of the data matrix $\bm D$, is what usually is zero
in cosmology, since we study quantities which are fluctuations around the mean and their mean is therefore identically
vanishing.
In the following sections we will write down the more specific and exact expressions for 
the Galaxy Clustering and Weak Lensing analysis.

\section{\label{sec:Fisher-Matrix-forecasts}Fisher Matrix forecasts}

The Fisher matrix formalism, which was mainly developed for cosmological observations by
\cite{tegmark_measuring_1998} and \cite{seo_baryonic_2005}
is one of the most popular tools to forecast the outcome of an experiment,
because of its speed and its versatility when the likelihood is approximately
Gaussian. 
The Fisher matrix method is used now ubiquitously in the cosmology literature, 
with about 200 papers published so far \footnote{According to a full-text search at https://www.arxiv.org}, ranging from CMB observations,
to Redshift Space Distortions, Supernovae, Lyman-$\alpha$ observations and many more.

We will present in the following sections, the specific formulas for Galaxy Clustering 
and Weak Lensing, under some simplifying assumptions (Gaussian likelihood,
diagonal data covariance matrices, no redshift-bin correlations).
In a paper by \cite{sellentin_breaking_2014} the Gaussian approximation has been 
dropped and higher order ``Fisher tensors`` are used to approximate the true underlying likelihood \cite{cite Sellentin more}.
Also recently, there has been some work in extending the simplifying assumptions of 
diagonal covariance, redshift-bin correlations and $k$-space correlations \cite{bailoni_improving_2016, Durrer, 
Amendola with Blot}.

In \cite{Khedekar, Majumdar} the authors also compared directly the Fisher matrix formalism with a direct MCMC approach
and have encountered considerable differences in the case of CMB experiments,
especially when departing from the standard cosmological model.

%Here we apply the Fisher matrix formalism to two different
%probes, Galaxy Clustering (GC) and Weak Lensing (WL), which are the
%main cosmological probes for the future Euclid satellite \cite{mukherjee_planck_2008}.
%The background and perturbations quantities we
%use in the following equations are computed with a version of
%\texttt{MGCAMB} \cite{zhao_searching_2009,hojjati_testing_2011}
%modified in order to account for the binning and the parameterizations
%described in Section \ref{sec:Parameterizing-Modified-Gravity}.

\subsection{Fisher matrix for Galaxy Clustering\label{sub:Fisher-Galaxy-Clustering}}

Using the previously found equation \ref{eq:Fisher-fromloglik-inPk} for the
Fisher matrix of the power spectrum, we can write that expression in a more compact form, where 
we express the sum over the $k$-modes as an integral and take into account the number of observed modes into
the volume. We end up with the following formula:
form \citep{seo_improved_2007, amendola book, dodelson}: 
\beeqc$\label{eq:fisher-matrix-gc}
F_{ij}=\frac{1}{8\pi^{2}}\int_{-1}^{+1}\mbox{d}\mu\int_{k_{\rm min}}^{k_{\rm max}}\mbox{d}k\,k^{2}\frac{\partial\ln
	\Pobs (k,\mu,z)}{\partial\theta_{i}}\frac{\partial\ln
	\Pobs (k,\mu,z)}{\partial\theta_{j}}V_{\rm eff}(k,\mu,z)
$
where the effective volume $V_{\rm eff}$ is given by
\beeqp$
V_{\rm eff} = V_{\rm survey}\left[\frac{n(z)\Pobs (k,\mu,z)}{n(z)\Pobs (k,\mu,z)+1}\right]^{2}
$
Here $V_{\rm survey}$
is the volume covered by the survey and contained in a redshift slice
$\Delta z$, while $n(z)$ is the galaxy number density as a function of
redshift. The galaxy number density has to be taken from specifications for each survey, which
are obtained by direct measurements in the sky and end-to-end simulations \cite{Dida, Guzzo, etc}.
$P_{obs}(k,\mu,z)$ is the observed galaxy power
spectrum as a function of redshift $z$, the wavenumber $k$ and
of $\mu\equiv\cos\alpha$, where $\alpha$ is the angle between the
line of sight and the 3D-wavevector $\vec{k}$. 
The derivatives in \cref{eq:fisher-matrix-gc} are taken with
respect to a vector of cosmological parameters, $\theta_{i}$ that can contain
redshift-independent parameters such as $h$ and $\Omega_{m}$ or redshift-dependent
parameters such as the growth rate function $f(z)$ or the Hubble function $H(z)$.
The details of the observed power spectrum will be treated in the next \cref{sub:observed-powerspectrum}.

The minimum and maximum values for the $k$-integral also depend on the survey specifications and
on how much we can trust and rely on non-linear scales. Throughout this work
we will use for
the smallest wavenumber a value of approximately $k_{\rm min}=0.008\textrm{h/Mpc}$, 
while the maximum wavenumber will depend on the specific application and methods used to study the non-linear regime.
In general, $k_{\rm max}={0.10-0.15}$h/Mpc for linear forecasts and for non-linear forecasts 
the smallest scales will lie at about $k_{\rm max}={0.5-1.0}$h/Mpc.

%we will use for our forecasts the 
%more standard recipe specified in the Euclid Redbook \cite{laureijs_euclid_2011}. 

 

\subsection{The observed galaxy power spectrum \label{sub:observed-powerspectrum}}

The distribution of galaxies in space is not perfectly uniform. Instead it follows, up to a bias, the underlying
matter power spectrum so that the observed power spectrum $P_{obs}$ 
is closely linked to the dark matter power spectrum $P(k)$. The observed
power spectrum is the Fourier transform of the real-space two point correlation
function (see \cref{eq:correlationfunct}) of the galaxy number overdensity.

The observed power spectrum is then built from the theoretical matter power spectrum
$P(k,z)$ (which in turn depends on the fundamental cosmological parameters
$\theta_{i}$) together by the following contributions which modify its signal:

\begin{enumerate}
\item The geometrical factor coming from the change of the Baryon Acoustic Peak (BAO) peak \\ marked orange
in \cref{eq:obs-power-basic}. It can be shown \cite{cite some BAO review} that the 
shift of the BAO peak due to geometrical distortions when considering a different cosmology
as the reference one is proportional to $H(z)$/$D^2_{A}(z)$.
\item The galaxy bias $b(z)$ marked brown in the formula below.
This term has to be estimated from observations and simulations
for different types of galaxy populations. In this case, we take into account
just a linear local bias which means that we assume that the galaxy overdensity $\delta_g$ differs from the 
underlying matter overdensity by a function which just depends on redshift: $\delta_g = b(z) \delta_m$.
This assumption breaks at very large and very small scales \cite{cite some bias review}.
\item The contribution from Redshift Space Distortions (RSD) \\
marked green in \cref{eq:obs-power-basic}, where $\beta(z) = f(z)/b(z)$, where
$b(z)$ is the galaxy bias and $f(z)$ the logarithmic growth rate of perturbations $f(z)=\mathrm{d}\ln(D_+)/\mathrm{d}\ln(a)/$.\\
This term is related to distortions in redshift space caused by peculiar velocity divergences. This term was derived first in
\citep{kaiser_clustering_1987} and it is commonly known as the Kaiser formula.
It represents the distortion caused by peculiar velocities when going from redshift to 
coordinate space.
\item The geometrical effect of the change of cosmological parameters
on the determination of the angles $\mu$ and the scale $k$, which is called the
Alcock-Paczynski effect \citep{alcock1979anevolution, ballinger_measuring_1996, feldman_power_1994} and it is marked red in \cref{eq:obs-power-basic}.
In \cref{sub:The-Alcock-Paczynski-Effect} we will detail the corresponding formulas.
\item The damping due to spectroscopic redshift errors $\sigma_{z}$ and the non-linear pairwise peculiar
velocity dispersion $\sigma_{v}(z)$, which corresponds to a first order correction
term to the Kaiser formula and it is also known as the "Fingers of God" effect. These terms damp the power spectrum at small scales, where due to these redshift
uncertainties the signal cannot be reproduced accurately. These terms are shown in magenta in \cref{eq:obs-power-basic}. 
\item Extra shot noise due to observational effects which cannot be removed. These can be included
in general with the term $P_{s}(z)$. It is marked blue in the formula below.
\end{enumerate}
\beeq$\label{eq:obs-power-basic}
P_{{\rm obs}}\left(z,k,\mu;\theta\right)=\textcolor{blue}{P_{{\rm s}}(z)}+{\color{orange}\frac{D_{A}^{2}(z)_{ref}H(z)}{D_{A}^{2}(z)H(z)_{ref}}}
{\color{brown}b^{2}(z)}
{\color{green}\left(1+\beta(z){\color{red}\mu}^{{\color{red}2}}\right)^{2}}P({\color{red}k},z){\color{magenta}e^{-k^{2}\mu^{2}(\sigma_{z}^{2}/H(z)+\sigma_{v}^{2}(z))}}
$
In the previous formula we have neglected relativistic contributions and further more detailed non-linear effects.
For a quick review on those topics see \cite{durrer, bonvin, taruya, scoccimarro}.
In general for the forecasts presented in this work,
we will marginalize over the bias $b(z)$, which we will take as a different nuisance parameter at each redshift. We usually fix
the spectroscopic redshift error  to a value given by the specifications of the instrument, and it is in general a very small number
$\sigma_{z}=0.001$. We also marginalize over $\sigma_{v}(z)$, 
since we don't have a proper way of computing it for each model and relating it to fundamental cosmological parameters.
We will take as a fiducial value $\sigma_{v} = 300$km/s, compatible with the estimates by \cite{de_la_torre_modelling_2012}.

Despite the apparent 
simplicity of \cref{eq:fisher-matrix-gc} and \cref{eq:obs-power-basic} there
are several details that need to be considered when dealing with forecasts
in practical applications. 
One of the main results of this work is the production of a Fisher Matrix code capable of performing
forecasts for Galaxy Clustering and Weak Lensing with different options for methods and assumptions.
In \cref{sec:FisherTools-code} we will explain more in detail the equations and the structure of the code
used to perform the forecasts in this work.


\subsection{Weak Gravitational Lensing \label{sub:Weak-Lensing-Fisher}}

Light propagating through the universe is deflected by variations in the Weyl potential $\Phi_{\textrm{Weyl}}=\Phi+\Psi$,
leading to distortions in the images of galaxies. 
The main idea behind weak gravitational lensing is to measure the ellipticity of galaxies in a 
wide field sky survey and correlate the ellipticities to find a statistical effect caused by the
evolution of the lensing potential in the Universe.

The transformation of a light bundle in the case of small angles and small gravitational potentials
can be described by a distortion tensor which contains in its diagonal the convergence component and in its
off-diagonal elements the shear component (see the comprehensive review by \cite{cite Bartelmann Schneider}).
It can be shown that for linear theory, the only component observable in cosmology
is the convergence field (see \cite{Amendola, Tegmark, Hu, Bartelmann}) 
and its power spectrum can be calculated once the matter power spectrum and the
evolution of the structures in the Universe in time are known.

In this regime we can write the power spectrum of the convergence field as
\begin{equation}
\label{def_shear}
C_{ij}(\ell)=\frac{9}{4}\int_{0}^{\infty}\mbox{d}z\:\frac{W_{i}(z)W_{j}(z)H^{3}(z)\Omega_{m}^{2}(z)}{(1+z)^{4}}\left[\Sigma(\ell/r(z),z)\right]P_{m}(\ell/r(z)) \, .
\end{equation}
In this expression we are already considered extensions of GR, 
where we have used Eqn.\ (\ref{eq:Sigma-def}) to relate the Weyl potential to $\Sigma$ and
to the matter power spectrum $P_m$. Furthermore we use the Limber approximation to write down the conversion $k=\ell/r(z)$, where $r(z)$ is the comoving
distance given by
\begin{equation}
r(z) = c\int_0^z \frac{d\tilde{z}}{H(\tilde{z})} \, .
\end{equation}
The indices $i,\:j$ stand for each of the $\mathcal{N}_{bin}$
redshift bins, such that $C_{ij}$ is a matrix of dimensions $\mathcal{N}_{bin}\times\mathcal{N}_{bin}$. The window functions $W_i$ are given by
\begin{equation}
W(z)=\int_{z}^{\infty}\mbox{d}\tilde{z}\left(1-\frac{r(z)}{r(\tilde{z})}\right)n(\tilde{z})
\end{equation}
where the normalized galaxy distribution function (in the case of a survey like Euclid \ref{tab:WL-specifications}) is
\begin{equation}
n(z)\propto z^{2}\exp\left(-(z/z_{0})^{3/2}\right) \, . \label{eq:ngal dist}
\end{equation}
Here the median redshift $z_{\rm med}$ and $z_{0}$ are related by $z_{\rm med}=\sqrt{2}z_{0}$.
We integrate this quantity in our redshift range to find the total amount of galaxies and
create the redshift bins, such that each of them contains the same number of galaxies. These bins
are then called equi-populated bins.
The Weak Lensing Fisher matrix is then 
given by a sum over all possible correlations at different redshift bins
\citep{tegmark_measuring_1998},
\begin{equation}
F_{\alpha\beta}=f_{\rm sky}\sum_{\ell}^{\lmax}\sum_{i,j,k,l}\frac{(2\ell+1)\Delta\ell}{2}\frac{\partial
C_{ij}(\ell)}{\partial\theta_{\alpha}}\textrm{Cov}_{jk}^{-1}\frac{\partial
C_{kl}(\ell)}{\partial\theta_{\beta}}\textrm{Cov}_{li}^{-1} \, . \label{eq:FisherSum-WL}
\end{equation}
The prefactor $f_{\rm sky}$ is the fraction of the sky covered by the survey. The upper limit of the sum, $\lmax$, is a high-multipole cutoff due to our ignorance of clustering and baryon physics on small
scales, similar to the role of $k_{\rm max}$ in Galaxy Clustering. In this work we choose $\lmax = 1000$ for the linear forecasts and $\lmax = 5000$ for the
non-linear forecasts (this cutoff is not necessarily reached at all multipoles $\ell$, as what matters is the minimum scale between $\ell_{\rm max}$ and $k_{\rm max}$, as we discuss below; see also \cite{casas_fitting_2015}).
In Eqn.\ (\ref{eq:FisherSum-WL}), $\textrm{Cov}_{ij}$ is the corresponding covariance matrix of the 
convergence power spectrum, which is the sum of the signal and the noise covariance matrix and it has the following form:
\begin{equation}
\textrm{Cov}_{ij}(\ell)=C_{ij}(\ell)+\delta_{ij}\gamma_{\rm int}^{2}n_{i}^{-1}+K_{ij}(\ell)
\end{equation}
where $\gamma_{\rm int}$ is the intrinsic galaxy ellipticity. In Table \ref{tab:WL-specifications} we cite some typical numbers for different surveys.
The shot noise term $n_{i}^{-1}$ is expressed as
\begin{equation}
n_{i}=3600\left(\frac{180}{\pi}\right)^{2}n_{\theta}/\mathcal{N}_{bin}
\end{equation}
with $n_{\theta}$ the total number of galaxies per $\text{arcmin}^2$ and the index $i$ standing for each redshift bin.
Since we have equi-populated redshift bins, the shot noise term is equal for each bin.
The matrix $K_{ij}(\ell)$
is a diagonal ``cutoff'' matrix, discussed for the first time in \cite{casas_fitting_2015} whose entries increase to very high
values at the scale where the power spectrum $P(k)$ has to be cut
to avoid the inclusion of uncertain or unresolved non-linear scales. We choose to add this matrix to have 
further control on the inclusion of non-linearities. Without this matrix, due to the redshift-dependent relation between
$k$ and $\ell$, a very high $\lmax$ would correspond at low redshifts, to a very high $k_{\rm max}$ where we do not longer
trust the accuracy of the non-linear power spectrum. Therefore, the sum in Eqn.\ (\ref{eq:FisherSum-WL}) is limited by the minimum scale imposed
either by $\lmax$ or by $k_{\rm max}$, which is the maximum wavenumber considered in the matter power spectrum $P(k,z)$. As we did for Galaxy Clustering, we use for linear forecasts $k_{\rm max}=0.15$ and for non-linear forecasts $k_{\rm max}=0.5$.



%\subsubsection{Weak Lensing \label{sub:Weak-Lensing} (FF)}
%
%For the weak lensing power spectrum the Fisher matrix is just a sum
%over all possible correlations at different redshift bins \citep{tegmark_measuring_1998},
%namely :
%
%
%
%\begin{equation}
%P_{ij}(\ell)=\frac{9}{4}\int_{0}^{\infty}\mbox{d}z\:\frac{W_{i}(z)W_{j}(z)H^{3}(z)\Omega_{m}^{2}(z)}{(1+z)^{4}}P_{m}(\ell/r(z))
%\end{equation}
%where $P_{m}$ is the matter power spectrum discussed above, the indices
%$i,\: j$ stand for each of the $\mathcal{N}$ redshift bins and the
%window functions are given by:
%
%\begin{equation}
%W(z)=\int_{z}^{\infty}\mbox{d}\tilde{z}\left(1-\frac{r(z)}{r(\tilde{z})}\right)n(\tilde{z})
%\end{equation}
%where the normalized galaxy distribution function is:
%
%\begin{equation}
%n(z)=z^{2}\exp\left(-(z/z_{0})^{3/2}\right)\label{eq:ngal dist}
%\end{equation}
%Here the median redshift and $z_{0}$ are related by $z_{med}\approx1.412z_{0}$.
%The corresponding covariance matrix has the following form:
%
%\begin{equation}
%C_{ij}(\ell)=P_{ij}(\ell)+\delta_{ij}\gamma_{int}^{2}n_{i}^{-1}+K_{ij}(\ell)
%\end{equation}
%where $\gamma_{int}$ is the intrinsic galaxy ellipticity and the
%shot noise term contains
%
%\begin{equation}
%n_{i}=3600\left(\frac{180}{\pi}\right)^{2}n_{\theta}/\mathcal{N}
%\end{equation}
%with $n_{\theta}$ the total number of galaxies per arcmin\texttwosuperior{}
%assuming that the redshift bins have been chosen such that each contain
%the same amount of galaxies (equi-populated bins). The matrix $K_{ij}(\ell)$
%is a diagonal ``cutoff'' matrix whose entries increase to very high
%values at the scales where the power spectrum $P(k)$ has to be cut
%to avoid the inclusion of numerical noise errors. Since $K_{ij}(\ell)$
%depends on the multipole $\ell$, which itself depends on $k$ and
%$z$ through: $\ell=k\, r(z)$, each of the entries $K_{ii}$ increase
%with the multipole in a different way. For our purposes we chose a
%very sharp cutoff, such that multipoles containing wavenumbers $k$
%beyond the Nyquist frequency (or half of the Nyquist frequency, for
%our reference case) at the center of the redshift bin $i$, do not
%contribute to the Fisher matrix at all after the corresponding $\ell_{cut}$
%(specified in table \ref{tab:WL-zbins-specs} below).




\subsection{Future large scale galaxy redshift surveys \label{sub:FutureSurveys}}

In this work we choose to present results on some of the future galaxy redshift surveys, which are planned to be started and analyzed within the next decade.
Our baseline survey will be the Euclid satellite \cite{amendola_cosmology_2013, laureijs_euclid_2011}. 

Euclid\footnote{http://www.euclid-ec.org/} is a European Space Agency medium-class mission scheduled for launch in 2020. 
Its main goal is to explore the expansion history of the Universe and the evolution of large scale cosmic structures by measuring shapes and redshifts of galaxies, covering 15000$\text{deg}^2$ of the sky, up to redshifts of about $z\sim2$. 
It will be able to measure up to 100 million spectroscopic redshifts which can be used for Galaxy Clustering measurements and 2 billion photometric galaxy images, which can be used for Weak Lensing observations (for more details, see \cite{amendola_cosmology_2013, laureijs_euclid_2011}). 
We will use in this work the  Euclid Redbook specifications for Galaxy Clustering and Weak Lensing forecasts \cite{laureijs_euclid_2011}, 
some of which are listed in Tables \ref{tab:GC-specifications} and \ref{tab:WL-specifications} and the rest can be found in the above cited references.

\begin{table}[h]
	\centering{}
	\begin{tabular}{|c|ccc|c|}
		\hline 
		\Tstrut \textbf{Parameter}  & \textbf{Euclid}  & \textbf{SKA1}  & \textbf{SKA2}  &
		\textbf{Description}\tabularnewline
		\hline 
		\Tstrut $f_{\rm sky}$  & 0.364  & 0.121  & 0.75 & Fraction of the sky covered\tabularnewline
		$\sigma_{z}$  & 0.05  & 0.05  & 0.05  & Photometric redshift
		error\tabularnewline
		$n_{\theta}$  & 30  & 10  & 2.7  & Number of galaxies per arcmin\tabularnewline 
		$\gamma_{\rm int}$  & 0.22  & 0.3  & 0.3  & Intrinsic galaxy ellipticity \tabularnewline
		$z_{0}$  & 0.9  & 1.0  & 1.6  & Median redshift over $\sqrt{2}$ \tabularnewline
		$\mathcal{N}_{bin}$ & 12 & 12 & 12 & Total number of tomographic redshift bins
		\tabularnewline
		\hline 
	\end{tabular}\caption[Specifications for future WL surveys.]{\label{tab:WL-specifications} Specifications for
		the Weak Lensing
		surveys Euclid, SKA1 and SKA2 used in this work. Other needed quantities can be found in the references cited in section \ref{sub:FutureSurveys}.
		For all WL surveys we use a redshift range between $z=0.5$ and $z=3.0$, using 6 equi-populated redshift bins.}
\end{table}

Another important future survey will be the Square Kilometer Array (SKA)\footnote{https://www.skatelescope.org/}, 
which is planned to become the world's largest radio-telescope. 
It will be built in two phases, phase 1 split into SKA1-SUR in Australia and SKA1-MID in South Africa and SKA2 which will be at least 10 times as sensitive. 
The first stage is due to finish observations around the year 2023 and the second phase is scheduled for 2030 
(for more details, see \cite{yahya_cosmological_2015,santos_hi_2015,raccanelli_measuring_2015,bull_measuring_2015}). 
The first phase SKA1, will be able to measure in an area of 5000$\text{deg}^2$ of the sky and a redshift of up to $z\sim0.8$  an estimated number of about $5\times10^6$ galaxies; SKA2 is expected to cover a much larger fraction of the sky ($\sim$30000$\text{deg}^2$), will yield much deeper redshifts (up to $z\sim2.5$) and is expected to detect about $10^9$ galaxies with spectroscopic redshifts \cite{santos_hi_2015}.
SKA1 and SKA2 will also be capable of performing 
radio Weak Lensing experiments, which are very promising, since they are expected to be less sensitive to systematic effects in the instruments, related to residual point spread function (PSF) anisotropies \cite{harrison_ska_2016}.
In this work we will use for our forecasts of SKA1 and SKA2, the specifications computed by \cite{santos_hi_2015} for GC and by \cite{harrison_ska_2016} for WL. The numerical survey parameters are listed in Tables \ref{tab:GC-specifications} and \ref{tab:WL-specifications}, while the galaxy bias $b(z)$ and the number density of galaxies $n(z)$, can be found in the references mentioned above.

We will also forecast the results from DESI\footnote{http://desi.lbl.gov/}, a stage IV, ground-based dark energy experiment, that will study large scale structure formation in the Universe through baryon acoustic oscillations (BAO) and redshift space distortions (RSD), using redshifts and positions from galaxies and quasars \cite{desi_collaboration_desi_2016-1,desi_collaboration_desi_2016,levi_desi_2013}.
It is scheduled to start in 2018 
and will cover an estimated area in the sky of about 
14000$\text{deg}^2$. It will measure spectroscopic redshifts for four different classes of objects, luminous red galaxies (LRGs) up to a redshift of $z=1.0$, bright [O II] emission line galaxies (ELGs) up to $z=1.7$, quasars (QSOs) up to $z\sim3.5$ and at low redshifts ($z\sim0.2$) magnitude-limited bright galaxies (BLGs). In total, 
DESI will be able to measure more than 30 million spectroscopic redshifts.
In this paper we will use for our forecasts only the specifications for the ELGs, as found in \cite{desi_collaboration_desi_2016-1}, since this observation provides the largest number density of galaxies in the redshift range of our interest. We cite the geometry and redshift binning specifications in Table \ref{tab:GC-specifications}, while the galaxy number density and bias can be found in \cite{desi_collaboration_desi_2016-1}.




\begin{table}[h]
	\centering{}
	\begin{tabular}{|c|cccc|c|}
		\hline 
		\Tstrut \textbf{Parameter}  & \textbf{Euclid}  & \textbf{DESI-ELG}  & \textbf{SKA1-SUR}
		& \textbf{SKA2}  & \textbf{Description}\tabularnewline
		\hline 
		\Tstrut $A_{\rm survey}$  & 15000 $\mbox{deg}^{2}$  & 14000 $\mbox{deg}^{2}$  & 5000
		$\mbox{deg}^{2}$  & 30000 $\mbox{deg}^{2}$  & Survey area\tabularnewline
		$\sigma_{z}$  & 0.001  & 0.001  & 0.0001  & 0.0001  & Spectroscopic
		error\tabularnewline
		$z_{\rm min,max}$  & \{0.65, 2.05\}  & \{0.65, 1.65\}  & \{0.05, 0.85\}
		& \{0.15, 2.05\}  & Min. and max. $z$ \tabularnewline
		$\Delta z$  & 0.1  & 0.1  & 0.1  & 0.1  & $\Delta z$ in bin\tabularnewline
		\hline 
	\end{tabular}\caption[Specifications for future GC surveys.]{\label{tab:GC-specifications} Specifications for
		the spectroscopic
		galaxy redshift surveys used in this work. The number density of tracers $n(z)$ and the galaxy bias $b(z)$, can be found for SKA in \cite{santos_hi_2015} and for DESI in reference \cite{desi_collaboration_desi_2016-1}.}
\end{table}






\subsection{Covariance and correlation matrix and the Figure of Merit\label{sec:covcorr}}

Previously we have defined the data covariance matrix in \cref{eq:dataCovariance-abstract}, now let us define in more generality the
parameter covariance matrix for a \emph{d-dimensional }vector
$p$ of model parameters as
\begin{equation}
\mathbf{C}=\langle \Delta p \Delta p^{T} \rangle\label{eq:covariance_def}
\end{equation}
with $\Delta p = p - \langle p \rangle$ and the angular brackets $\langle \, \rangle$ representing an expectation value.
For our Fisher matrix analysis we will assume that $\langle p \rangle$ is the value of the parameter $p$ that maximizes the likelihood.
The matrix $\mathbf{C}$, with all its off-diagonal elements set to zero, is called the variance matrix $V$ and contains the
square of the errors $\sigma_{i}$ for each parameter
$p_{i}$
\begin{equation}
\mathbf{V} \equiv diag(\sigma_{1}^{2},...,\sigma_{d}^{2})\,\,\,.
\end{equation}
The Fisher matrix $\mathbf{F}$ is the inverse of the parameter covariance matrix
\begin{equation}
\mathbf F= \mathbf C^{-1}\,\,\,.
\end{equation}
The covariance matrix tells us not only the errors on each of the parameters $p_i$,
but also how the errors are correlated among each other. This is more clearly seen by defining
the correlation matrix $\mathbf P$, which is obtained from the covariance matrix
$\mathbf C$, in the following way
\begin{equation}
P_{ij}=\frac{C_{ij}}{\sqrt{C_{ii}C_{jj}}} \, . \label{eq:correlation_def}
\end{equation}
If the covariance matrix is non-diagonal, then there are correlations
among some elements of $p$. We can observe this also by plotting
the ellipsoidal contours corresponding to the confidence regions. The orientation of the ellipses can
tell us if two variables $p_{i}$ and $p_{j}$ are correlated ($P_{ij}>0$),
corresponding to ellipses with 45 degree orientation to the right of the vertical
line or if they are anti-correlated ($P_{ij}<0$), corresponding to ellipses oriented 45
degrees to the left of the vertical line.

To summarize the information contained in the Fisher/covariance matrices we can
define a Figure of Merit (FoM).
%Here we choose the logarithm of the determinant, while another possibility would
%be the Kullback-Leibler divergence, which is a measure of the information entropy gain, see Appendix \ref{sec:KL}.
The square-root $\sqrt{\det(\mathbf{C})}$ of the determinant of the covariance matrix is proportional to the volume of the error
ellipsoid. We can see this if we rotate our coordinate system so that the 
covariance matrix is diagonal, $\mathbf C = {\rm diag}(\sigma_1^2, \sigma_2^2, \ldots \sigma_{d}^{2})$, then $\det(\mathbf C) = \prod_i \sigma_i^2$ and $(1/2)\ln(\det(\mathbf C)) = \ln \prod_i\sigma_i$ would indeed represent the logarithm of an error volume. Thus, the smaller the determinant (and therefore also $\ln(\det(\mathbf{C}))$), the smaller is the ellipse and the stronger are the constraints on the parameters. We define
\begin{equation}\label{eq:FoM}
\mathrm{FoM} = -\frac{1}{2} \ln(\det(\mathbf{C})) \, ,
\end{equation}
with a negative sign in front such that stronger constraints lead to a higher Figure of Merit. 
%In the following, the value of the FoM reported in all tables will be obtained including only the dark energy parameters (i.e.\ the $(\mu_i,\eta_i)$ sub-block for the binned case and the $(\mu,\eta)$ sub-block in the smooth functions case), after marginalizing over all other parameters. 
The FoM allows us to compare not only the constraining power of different probes but also of the different experiments. 
As the absolute value depends on the details of the setup, 
we can define the relative figure of merit between probe $a$ and probe $b$: $\mathrm{FoM}_{a,b} = -1/2 \ln(\det(\mathbf{C_a})/\det(\mathbf{C_b})) = \mathrm{FoM}_{a}-\mathrm{FoM}_{b}$.
%and we fix our reference case (probe $b$), for each parametrization, to the Galaxy Clustering observation using linear power spectra with the Euclid survey (labeled as `Euclid Redbook GC-lin' in all figures and tables). 
The FoM has units of `nits', since we are using the natural logarithm. 
These are similar to `bits', but `nits' are counted in base $e$ instead of base 2.

An analogous construction allows us to study quantitatively the strength of the correlations encoded by the correlation matrix $\mathbf P$.
We define the `Figure of Correlation' (FoC) as:
\begin{equation}\label{eq:FoC}
\mathrm{FoC} = -\frac{1}{2} \ln(\det(\mathbf{P})) \, .
\end{equation}
If the parameters are independent, i.e.\ fully decorrelated, then $\mathbf{P}$ is just the unit matrix and $\ln(\det(\mathbf{P}))=0$. 
Off-diagonal correlations will decrease the logarithm of the determinant, therefore making the FoC larger. 
From a geometrical point of view, the determinant expresses a volume spanned by the vector of (normalized) variables. 
If these variables are independent, 
the volume would be maximal and equal to one, while if they are strongly linearly dependent, the volume would be squeezed 
and in the limit where all variables are effectively the same, the volume would be reduced to zero. 
Hence, a more positive FoC indicates a stronger correlation of the parameters.

\subsection{The Kullback-Leibler divergence \label{sub:KL-def}}

In the previous \cref{sec:covcorr} we exploited the determinant of the covariance matrix $C$ as a Figure of Merit for our forecasts.
Here we summarize a possible alternative to measure the constraining power of a specific forecast, 
namely the Kullback-Leibler divergence \cite{kullback_information_1951}, also called relative entropy or information gain. 
It has been used in the field of cosmology for model selection, experiment design and forecasting, see among others \cite{kunz_constraining_2006, raveri_cosmicfish_2016, seehars_information_2014, verde_planck_2013, Zhao2017}.
The KL-divergence $\mathcal{D}(p_2||p_1)$ measures for a continuous, $d$-dimensional random variable $\theta$, the relative entropy between two probability density functions $p_1(\theta)$ and $p_2(\theta)$ and it is given by
\begin{equation}
\mathcal{D}(p_2||p_1) \equiv \int p_2(\theta)\ln\left(\frac{p_2(\theta)}{p_1(\theta)} \right) \mathrm{d}\theta .
\end{equation}
Although it is not symmetric in $p_1$ and $p_2$ it can be interpreted as a distance between the two distributions and measures the information gain since it is non-negative ($\mathcal{D}(p_2||p_1)\geq0$), non-degenerate ($\mathcal{D}(p_2||p_1)=0$ if and only if $p_1=p_2$) and it is invariant 
under re-parameterizations of the distributions as $p_1(\theta)\mathrm{d}\theta = p_1(\tilde{\theta})\mathrm{d}\tilde{\theta}$. 
In the form given here the information gain is measured in nits as in section \ref{sec:covcorr}, 
to convert nits to bits it is enough to divide the result by $\ln(2)$.

For the special case of $p_1(\theta)$ and $p_2(\theta)$ being multivariate Gaussian distributions,
with the same mean values and covariance matrices $\mathcal{A}$ and $\mathcal{B}$ respectively, we obtain
\begin{equation}\label{eq:KL-Gaussian}
\mathcal{D}(p_2||p_1) = -\frac{1}{2} \left[\ln\left(\frac{\det(\mathcal{A})}{\det(\mathcal{B})} \right) +\mathrm{Tr}\left[\mathbbm{1}-\mathcal{B}^{-1}\mathcal{A}   \right] \right].
\end{equation}
We can then define a Kullback-Leibler matrix $\mathcal{K}$, introduced in (\cite{casas_mg_forec} appendix F) composed of the KL-divergence measure among our observables, defined as:
\begin{equation}\label{eq:KL-Matrix}
\mathcal{K}_{ij}=\mathcal{D}(p_j||p_i).
\end{equation}
where $p_i$ and $p_j$ represent our observables.
By looking at the rows of this matrix (one row for each observable) and plotting them
in a corresponding matrix plot, one can see graphically which combination
of observables yield more information gain.
In \cref{sub:KL-matrices-MGBin} we will show the visual representation
of the KL-matrix for an explicit parameterization of modified gravity
and for different Galaxy Clustering and Weak Lensing combinations of observables.
This will give an intuitive idea of which observable contains more information gain than the other, 
compared to a reference observable.

\subsection{Systematic bias on cosmological parameters \label{sub:syst-bias-theory}}

As we have seen in previous sections, our forecasts for future observations
depend strongly on the knowledge of the theoretical matter power spectrum.
For linear scales, this can be calculated with great precision, but for non-linear scales, 
we have to rely on N-body simulations, semi-analytic methods or perturbation theories. Each 
of these methods only have a certain range of validity and introduce many numerical and theoretical 
errors into the calculations. We will see more details on this topic in \cref{chap:nonlinear}.

In this section we will quantify the effect of the systematic errors on our forecasted statistical error estimations
due to the uncertainties on the non-linear power spectrum. We will
show how big this systematic bias would be, if we used for our forecasts
a power spectrum which is not the ``correct'' one. 
% for example one obtained from a single N-body realization.
The following discussion was introduced in \cite{casas_fitting_2015} in reference to
forecasts using non-linear power spectra from N-body simulations and is based mostly on 
the expressions derived in Appendix
B of \cite{taylor_probing_2007}.

The linear bias on a cosmological parameter $\delta\theta_{i}$ due
to the bias $\delta\psi_{i}$ in a parameter of the model which we
assume fixed and cannot be measured is given by: \todo[Comp.]{Add derivation of this equation}
\begin{equation}
\delta\theta_{i}=-\left[F^{\theta\theta}\right]_{ik}^{-1}F_{kj}^{\theta\psi}\delta\psi_{j}\label{eq:syst.bias}
\end{equation}
In our case we will have only one systematic parameter $\psi$, which
controls the difference between the ``true'' power spectrum $P_{true}$
and and our simulated power spectrum $P_{num}$:

\[
P_{\psi}=\psi P_{num}+(1-\psi)P_{true}
\]
$\psi$ can vary continuously so that for $\psi=1$ we recover $P_{num}$,
while for $\psi=0$ we obtain $P_{true}$. We can define 
the relative difference between $P_{true}$ and $P_{num}$ as:
\begin{equation}
\sigma_{p}(k,z)\equiv\frac{P_{num}(k,z)-P_{true}(k,z)}{P_{true}(k,z)}
\end{equation}


The $F^{\theta\theta}$ in eqn.\ref{eq:syst.bias} above is simply
the usual Fisher matrix: 
\[
F^{\theta\theta}=\frac{1}{2}\mbox{tr}\left(C^{-1}\partial_{i}^{\theta}CC^{-1}\partial_{j}^{\theta}C\right)
\]
while the pseudo-Fisher matrix between measured and assumed parameters
$F^{\theta\psi}$ is:

\begin{equation}
F_{ij}^{\theta\psi}=\frac{1}{2}\mbox{tr}\left(C^{-1}\partial_{i}^{\theta}CC^{-1}\partial_{j}^{\psi}C\right)
\end{equation}
which for one systematic parameter only, is just a column vector.

In the case of galaxy clustering we will compute $F^{\theta\psi}$
in the following way, using the fact that for $\psi=1$, $C=P_{num}(k,z)+n^{-1}(z)$
and $P_{\psi}|_{\psi=1}=P_{num}$:

\beeq$ \label{eq:pseudo-Fisher}
F_{i}^{\theta\psi}\propto\int\mbox{d}k\, k^{2}\left(\frac{n_{eff}(z)P_{num}(k,z)}{n_{eff}(z)P_{num}(k,z)+1}\right)^{2}\left(\frac{1}{P_{num}(k,z)}\right)^{2}\left.\frac{\partial P_{\psi}}{\partial\psi}\right|_{\psi=1}\frac{\partial P_{num}}{\partial\theta_{i}}
$
in this step we have assumed that we have no systematic parameters
affecting the galaxy number density $n(z)$ and therefore, its derivative
w.r.t $\psi$ vanishes. Also, for notational simplicity we left out the integral over $\mu$ and the complete
form of the observed power spectrum.

In \cref{sub:syst-bias-nbodypk} we will show a concrete example of the application of these formulas
and we will see that in the non-linear regime, where the uncertainty in 
the theoretical power spectrum is of about 10-20\%, the systematic
errors can be of the same order or larger than the forecasted statistical errors for a future survey like Euclid.


\section{The equations and structure of the \textsc{FisherTools} code \label{sec:FisherTools-code}}

In this section we will explain a bit more in detail the 
implementation of the  \fito code, which has been used in the projects and papers
explained throughout this work.
This code was created by the author and consists on a set of packages for the \texttt{Mathematica Wolfram}
language.

\subsection{The structure of \fito \label{sub:fishertools}}

The code contains of 4 basic modules:
\begin{enumerate}
	\item CosmologyTools: A package defining many background quantities in cosmology, such as the Hubble function
	$H(z)$, distances, volumes, density fractions $\Omega(z)$, the growth and growth rate functions $D_{+}$ and $f(z)$,
	conversions between different units and time conventions (scale factor, redshift, e-fold time, conformal time) and functions
	related to observational effects, like RSD and the AP effect (see \cref{sub:observed-powerspectrum}), among others.
	This package also interacts with the package \textsc{Cosmomathica} (see \cref{sub:cosmomathica} below), which 
	serves as an interface and a wrapper to many other useful codes in the community,
	especially Boltzmann codes.
	\item FisherTools: A package containing all functions needed to perform and analyze a Fisher forecast. Many of them
	are too technical to be included here and belong to a comprehensive manual, but in the next subsection we will specify some of them.
	The most important ones are the routines for derivation and integration, together with routines for matrix operations and 
	visualization.
	\item WeakLensingTools: Some special functions related to Weak Lensing analysis, like window functions, multipoles,
	computation of bins and bin correlations.
	\item UsefulTools: Auxiliary tools used in the code that are not provided by \texttt{Mathematica},
	such as file exporting and importing, string parsing and matrix operations.
\end{enumerate}

\subsection{The interface \textsc{Cosmomathica} \label{sub:cosmomathica}}

The \textsc{Cosmomathica} code was created originally by Dr. Adrian Vollmer and used 
for several projects in the field of Fisher matrix forecasts and perturbation theory \cite{papers by Adrian}.
It is now maintained by the author of this work and it can be found 
and copied from its repository under the URL: \url{https://github.com/santiagocasas/cosmomathica}.

Its main function is to interface many codes and routines used in the cosmology community to a simple \texttt{Mathematica} notebook
or package.
The supported codes are the Boltzmann codes: \texttt{CAMB} \cite{lewis_efficient_2000} and \texttt{CLASS} \cite{lesgourges}.
The fitting functions from Eisenstein \& Hu \cite{cite Eisenstein}, \texttt{Halofit} \cite{smith} and the \texttt{Cosmic Emulator}
\cite{heitmann_coyote_2010, heitmann_coyote_2014}; and a code that calculates higher order perturbation theory for large scale structure, 
called \texttt{COPTER} \cite{find citation}.

This interface (which uses the \texttt{MathLink} technology from the \texttt{Wolfram} language) allows a very efficient and fast communication
between the notebooks, the packages and the codes. Therefore, there is no need to create large quantities of files that have to be read
and written in order to deal with cosmological quantities like the power spectrum or the transfer functions.


\subsection{Derivatives of the observed power spectrum}

The main calculation done in this code for galaxy clustering is based on \cref{eq:fisher-matrix-gc}, 
which involves derivatives of the observed power spectrum and an integral over wavevectors $k$ and angles $\mu$.
For each redshift bin $n$, the derivatives are evaluated at the center of each bin $\bar{z}_n$ and 
at the fiducial values of the parameters. The total number of redshift bins is $N_b$.

Let us first consider the derivatives of the observed power spectrum (see \cref{eq:obs-power-basic}) 
with respect to a number $N_p$ of redshift-independent parameters, which we will
denote $\theta_i$, where the index $i$ runs from 1 to $N_p$.
These are parameters of the model which are fixed numbers like $n_s$, or are defined at redshift $z=0$, like $\Omega_m$.
In this case there are two possible options in terms of code implementation.

\textbf{1. Full Numerical Derivative method:}
	\begin{equation}
	\left.\frac{\mbox{d}\ln P_{{\rm obs}}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\mbox{d}\theta_{i}}\right|_{fid}=\frac{P_{{\rm obs}}\left(\bar{z}_n,k,\mu;\theta_{i}^{+}\right)-P_{{\rm obs}}\left(\bar{z}_n,k,\mu;\theta_{i}^{-}\right)}{2\varepsilon\theta_{i}^{fid}\times P_{{\rm obs}}\left(\bar{z}_n,k,\mu;\theta_{i}^{fid}\right)}
	\end{equation}
	where $\theta_{i}^{+}$, $\theta_{i}^{-}$ represent the parameter
	$\theta_{i}$ evaluated at $\pm\varepsilon$ around the fiducial value
	$\theta_{i}^{fid}$:
	\begin{equation}
	\theta_{i}^{\pm}=\theta_{i}^{fid}(1\pm\varepsilon)
	\end{equation}
Almost all functions inside $P_{{\rm obs}}(\bar{z},k,\mu;\theta_{i})$
can be evaluated at $\theta_{i}^{\pm}$, except for the bias function $b(z)$,
which we cannot relate to the cosmological parameters. Therefore, we need to consider 
for each redshift bin, an independent parameter $b_n = b(\bar{z}_n)$. This can be 
considered a redshift-dependent parameter.

\textbf{2. Chain Rule method:}

The observed power spectrum (\cref{eq:obs-power-basic}) depends on 5 functions of the redshift, which we can 
call redshift-dependent variables.
However, just three of them,  $H(z)$, $D_A (z)$ and $f(z)$ depend on the cosmological parameters
$\theta_i$.
Then, with the help of the chain rule one can write:
\begin{align}\label{eq:derivatives-of-lnPobs} 
\left.\frac{\mbox{d}\ln P_{{\rm obs}}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\mbox{d}\theta_{i}}\right|_{fid} 
%& =\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\ln P_{s}(\bar{z}_n)}\frac{\partial\ln P_{s}(\bar{z}_n)}{\partial\theta_{i}} \\
& =\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\ln f(\bar{z}_n)}\frac{\partial\ln f(\bar{z}_n)}{\partial\theta_{i}} \\
& +\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\ln H(\bar{z}_n)}\frac{\partial\ln H(\bar{z}_n)}{\partial\theta_{i}} \nonumber \\ 
& +\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\ln D_{A}(\bar{z}_n)}\frac{\partial\ln D_{A}(\bar{z}_n)}{\partial\theta_{i}}\nonumber \\
& +\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\ln P(k,\bar{z}_n)}\frac{\partial\ln P(k,\bar{z}_n)}{\partial\theta_{i}} \nonumber \\ 
%& +\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\ln b(\bar{z}_n)}\frac{\partial\ln b(\bar{z}_n)}{\partial\theta_{i}}\nonumber\\
& +\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial k}\frac{\partial k}{\partial\theta_{i}} \nonumber \\
& +\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\mu}\frac{\partial\mu}{\partial\theta_{i}}\nonumber 
\end{align}
The last two terms, which consider derivatives of $k$ and $\mu$ with respect to $\theta_i$ are non-vanishing
if one takes the Alcock-Paczynski effect into account for $k$ and
$\mu$, since they are affected by geometrical terms, we will detail their expressions in \cref{sub:The-Alcock-Paczynski-Effect} below.
The redshift dependent functions $b(z)$ and $P_s (z)$ are not known as a function of the fundamental cosmological parameters,
and their functional form as a function of redshift is also generally unknown. Therefore, the best we can do is to
discretize them in redshift bins and assign some fiducial values for them. We have to assume that each value at each bin is independent of the other.
We we will therefore have $2 \times N_b$ unknown parameters in the observed power spectrum, namely $b_n = b(\bar{z}_n)$ and $P_{s,n} = P_{s}(\bar{z}_n)$,
the values of the bias and the extra shot noise at $\bar{z}_n$.
In order to simplify the resulting equations, we will use the natural logarithm of these quantities.
Then, these derivatives are:
\begin{align}\label{eq:derivatives-of-lnPobs-dPs} 
\left.\frac{\mbox{d}\ln P_{{\rm obs}}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\mbox{d} P_{s}(\bar{z}_n)}\right|_{fid} 
& =\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial \ln P_{s,n}} \nonumber \\
& =\frac{1}{P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}
\end{align}
where we have assumed the fiducial extra shot noise value to be $P_{s,n} = 0$ at all bins, and
\begin{align}\label{eq:derivatives-of-lnPobs-dbias} 
\left.\frac{\mbox{d}\ln P_{\mathrm{obs}}\left( \bar{z}_n , k, \mu; \theta_{i} \right) }{ \mbox{d} \ln b(\bar{z}_n) }\right|_{fid} 
& =\frac{\partial\ln P_{obs}\left( \bar{z}_n,k,\mu; \theta_{i} \right) }{ \partial \ln b_{n} } \nonumber \\
& =\frac{2}{1+\beta(\bar{z}_n)\mu^{2}}
\end{align}
Therefore, our set of parameters extends from $\theta_i$ to $\Theta_i = \{ \theta_i, \ln b_{n}, \ln P_{s,n} \}$.
The Fisher matrix \cref{eq:fisher-matrix-gc} will depend now on all the combinations of the first derivatives of the power spectrum
with respect to $\Theta$ and will be of the dimensions $(N_p + 2 N_b) \times (N_p + 2 N_b)$, since we have 2 redshift-dependent
parameters at $N_b$ redshift bins and $N_p$ cosmological redshift-independent parameters.

The intermediate derivatives of $\ln P_{obs}(z,k,\mu;\theta_{i})$
with respect to $D_{A}(z)$, $H(z)$ and $f(z)$ can be calculated analytically from
\cref{eq:obs-power-basic}. We use the logarithm of these quantities,
because it simplifies considerably the formulas. As usual, the derivatives are calculated
at the \emph{fiducial} value of the cosmological parameters.
\begin{subequations}	
	\begin{align}
	%\frac{\partial\ln P_{obs}\left(\bar{z},k,\mu;\theta_{i}\right)}{\partial\ln P_{s}(\bar{z})} & =\frac{1}{P_{obs}\left(\bar{z},k,\mu;\theta_{i}\right)}\\
	\frac{\partial\ln P_{obs}\left(\bar{z},k,\mu;\theta_{i}\right)}{\partial\ln f(\bar{z})} & =\frac{2\beta(\bar{z})\mu^{2}}{1+\beta(\bar{z})\mu^{2}}\\
	%\frac{\partial\ln P_{obs}\left(\bar{z},k,\mu;\theta_{i}\right)}{\partial\ln b(\bar{z})} & =\frac{2}{1+\beta(\bar{z})\mu^{2}}\\
	\frac{\partial\ln P_{obs}\left(\bar{z},k,\mu;\theta_{i}\right)}{\partial\ln H(\bar{z})} & =1 + \mathrm{AP}_{\mathrm{H}} \\
	\frac{\partial\ln P_{obs}\left(\bar{z},k,\mu;\theta_{i}\right)}{\partial\ln D_{A}(\bar{z})} & =-2 + \mathrm{AP}_{\mathrm{D}} \\
	\frac{\partial\ln P_{obs}\left(\bar{z},k,\mu;\theta_{i}\right)}{\partial\ln P(k,\bar{z})} & = 1
	\end{align}
	\label{eq: partial-derivs-subeqns}	
\end{subequations}
Here, the $\mathrm{AP}_{\mathrm{H}}$ and $\mathrm{AP}_{\mathrm{D}}$ represent the extra terms appearing from the Alcock-Paczynski 
effect, where the observed $k$ and $\mu$ are corrected by geometrical terms. These formulas will be specified in \cref{sub:The-Alcock-Paczynski-Effect} below.

%If for the moment and for simplicity we neglect the extra shot noise
%contribution $P_{s}$ and the dependence of $k$ and $\mu$ on the
%change of cosmological parameters (see AP-effect section below) we
%get for each derivative of $P_{obs}(k,\mu,z)$with respect to a cosmological
%parameter $\theta_{i}$:
%
%\begin{align}
%\left.\frac{\mbox{d}\ln P_{{\rm obs}}\left(\bar{z},k,\mu;\theta_{i}\right)}{\mbox{d}\theta_{i}}\right|_{fid} & =\frac{\partial\ln P(k,\bar{z})}{\partial\theta_{i}}+\frac{\partial\ln H(\bar{z})}{\partial\theta_{i}}-2\frac{\partial\ln D_{A}(\bar{z})}{\partial\theta_{i}}\nonumber \\
%& +\frac{2\beta(\bar{z})\mu^{2}}{1+\beta(\bar{z})\mu^{2}}\frac{\partial\ln f(\bar{z})}{\partial\theta_{i}}+\frac{2}{1+\beta(\bar{z})\mu^{2}}\frac{\partial\ln b(\bar{z})}{\partial\theta_{i}}\label{eq:derivatives-of-lnPobs-simplified-1}
%\end{align}

\textbf{3. The BAO method:}

The third option would be to consider first the redshift-dependent functions to be independent of the cosmological
parameters $\theta_i$ and constrain them independently of the cosmological model.
This is the preferred option for observational cosmologists, since 
it is more connected to the actual observations and it is more model independent.
In this method, one simply calculates \cref{eq:derivatives-of-lnPobs} ignoring the derivatives
of the redshift-dependent functions with respect to $\theta_i$.
Using these three redshift-dependent variables, together with the other two mentioned previously, $b(z)$
and $P_s(z)$, the space of parameters grows to
\beeqc$
 \Theta_j = \{ \theta_i, \ln b_{n}, \ln P_{s,n}, H_n, D_{A,n}, f_n \}
$
where the subscript $n$, corresponds to the function evaluated at $\bar{z}_n$, the center of the redshift bin.
In this case the Fisher matrix would be much bigger, with
$(N_p + 5 N_b) \times (N_p + 5 N_b)$ elements,  with $N_b$ the number of redshift bins 
and $N_p$ the number of redshift-independent parameters.
If one wishes to project the redshift-dependent parameters into the fundamental cosmological
parameters, one simply calculates a Jacobian of the "old" variables with respect to the new variables, 
where the new fundamental cosmological parameters $\tilde{\theta}_i$ can differ from the "old"
variables and leaves open the opportunity of changing the model parameter basis:
\beeqp$
J_{a b} = \parder{\Theta_a}{\tilde{\theta}_b}
$
Notice that if $\tilde{\theta}_i = \theta_i$, the first $(N_p) \times (N_p)$ elements 
of the Jacobian matrix will be a unit matrix. 
This Jacobian is a non-square matrix since $\Theta_a$ contains $ (N_p + 5 N_b)$ components and
$\tilde{\theta}_b$ contains just $ (N_p)$ components,
therefore it cannot be inverted.
The new Fisher matrix would change accordingly to $\tilde F$ and it would be given by:
\begin{equation}
\tilde F = J^{T}\, F \,J
\end{equation}

This method is called the "BAO" method \cite{seo_improved_2007, seo_improved_2005}, since in BAO observations
the redshift independent functions $D_{A}(z)$, $H(z)$ and $f(z)$ are the main observables and
one obtains cosmological parameter constraints from them by first constraining their values 
and then projecting these constraints onto a cosmological model. 
However, since the bias (which also enters $\beta(z)$) is degenerate with the
overall amplitude of the power spectrum as a function of redshift, $\sigma_8 (z)$, observers prefer to define an observed power spectrum
which depends on $f\sigma_{8}(z)$ and $b \sigma_{8}(z)$, so that instead of \cref{eq:obs-power-basic}, the observed power spectrum would be expressed as
(ignoring the exponential damping term for simplicity, which remains unaltered anyway):
\beeqp$\label{eq:obs-power-fs8}
P_{{\rm obs}}\left(z,k,\mu;\theta\right)=P_{{\rm s}}(z)+\frac{D_{A}^{2}(z)_{ref}H(z)}{D_{A}^{2}(z)H(z)_{ref}}
\left(b\sigma_{8}(z)+f\sigma_{8}(z)\mu^{2}\right)^{2} \left(\frac{P(k,z)}{\sigma_{8}^{2}(z)}\right)
%e^{-k^{2}\mu^{2}(\sigma_{z}^{2}/H(z)+\sigma_{v}^{2}(z))}
$

\subsection{Equations for the power spectrum}

Let us review here some of the formulas related to the power spectrum, in the way they are implemented in the \fito 
code. 

The normalization
$\sigma_{R}$ of the power spectrum at $z=0$ (which is equivalent to the variance
smoothed over a scale $R$) is given by:
\beeqp$\label{eq:normalizationPk}
\sigma_{R}^2=\frac{1}{2\pi^{2}}\int_{k_{min}}^{k_{max}}k^{2}P(k,z=0)W_{R}^{2}(kR)
$
The window function smooths $P(k,z)$ over the scale $R$ and has
the form: $W(x)=3(\sin(x)-x\cos(x))/x^{3}$, where $x$ is a dimensionless
variable. This term comes from assuming spherical symmetry and performing
the angle integration of the 3-D power spectrum $P(\vec{k})$. If
one chooses $R=8\mbox{h}^{-1}\mbox{Mpc}$ and $[k]=\mbox{h}/\mbox{Mpc}$,
then we can define the parameter  
\beeqp$
\sigma_{8}^{2} \equiv \sigma_{R}^2 \quad (R=8\textrm{Mpc/h})
$
The quantity
$\sigma_{8}$ is a parameter defined always at $z=0$ and only in linear theory, which means
that in \cref{eq:normalizationPk} either $P(k,z=0)$ is the linear power spectrum
or the integration limits are chosen in such a way that only linear
scales are taken into account, therefore one should take
$k_{max}\approx0.1$. 

The linear power spectrum can be obtained either from \texttt{CAMB}, \texttt{CLASS}
or from the transfer functions of Eisenstein \& Hu. In the latter case,
we can express the power spectrum $P(k,z)$
in terms of the primordial amplitude $\mathcal{A}_{s}$, the linear growth $G(z)$ (equivalent to $D_{+}(a)$ in alternative notations) and
the transfer functions $T(k)$: 
\beeqp$
P(k,z)= G^{2}(z)T^{2}(k)k^{n_s}\mathcal{A}_{s}
$
Since the parameters $\sigma_{8}^{2}$ and $\mathcal{A}_{s}$ are fully degenerate in linear theory,
one has to be careful to include only one of them into the set of cosmological parameters.
The quantity $\sigma_{8}(z) \equiv \sigma_{8}^{2}G^{2}(z)$
can be defined accordingly and can be used as an independent and redshift-dependent
cosmological parameter, as it was the case in \cref{eq:obs-power-fs8}. 
We have to emphasize again that these quantities and expressions are only formally 
valid in the linear regime and in standard Einstein GR;
in the deeply non-linear regime or under the effect of modified gravity forces,
the growth function might acquire some scale-dependence.


\subsection{The Alcock-Paczynski Effect\label{sub:The-Alcock-Paczynski-Effect}}

The Alcock-Paczynski effect (AP, see \citep{alcock1979anevolution}) relies on the fact that the scales
$k$ and the angle cosines $\mu$ are changed by geometrical factors
of distance, when the cosmological parameters are changed. This means
that $k$ and $\mu$ depend on $H(z)$ and $D_{A}(z)$ and therefore
are also indirectly functions of the cosmological parameters $\theta_{i}$.
The AP formulas relating the fiducial values of $k_{\rm fid}$ and $\mu_{\rm fid}$
to their transformed values are:

\begin{align}
k_{AP} & = R_{AP}(\mu;\theta)k_{\rm fid}\\
\mu_{AP} & =\frac{H(z;\theta)}{H(z;\theta_{\rm fid})}\frac{\mu_{\rm fid}}{R_{AP}(\mu;\theta)}
\end{align}
where the geometrical function $R_{AP}$ is defined as:
\beeqp$
R_{AP}(\mu;\theta)=\sqrt{\frac{[D_{A}(z;\theta)H(z;\theta)\mu_{fid}]^{2}-[(D_{A}(z;\theta_{\rm fid})
		H(z;\theta_{\rm fid})]^{2}(\mu_{\rm fid}-1)}{[D_{A}(z;\theta)H(z;\theta_{\rm fid})]^{2}}}
$
The last two terms of \cref{eq:derivatives-of-lnPobs} come from
the fact that now $k=k(H(z;\theta),D_{A}(z;\theta))$ and $\mu=k(H(z;\theta),D_{A}(z;\theta))$,
so that one has to use the chain rule for the derivative of $P_{obs}(k,\mu,z;\theta)$:
\begin{align}
\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;H(z);\theta_{i}\right)}{\partial\ln H(z)} 
& =\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\ln H(z)}\\
& +\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\mu}\frac{\partial\mu}{\partial\ln H(z)} \nonumber \\ 
& +\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial k}\frac{\partial k}{\partial\ln H(z)}\nonumber \quad,
\end{align}
and similarly for the dependence on $D_{A}(z)$.

Now we can write down explicitly the intermediate derivatives (evaluated
at the fiducial value as usual):
\begin{equation}
\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial k}=\frac{\partial\ln P\left(\bar{z}_n,k;\theta_{i}\right)}{\partial k}
\end{equation}


\begin{equation}
\frac{\partial\ln P_{obs}\left(\bar{z}_n,k,\mu;\theta_{i}\right)}{\partial\mu}=\frac{4\beta(z)\mu}{1+\beta(z)\mu^{2}}
\end{equation}
and the derivatives of $k$ and $\mu$ with respect to $\ln D_{A}$
and $\ln H$ are:

\begin{subequations}
	
	\begin{align}
	\frac{\partial\mu}{\partial\ln D_{A}(z)} & =-\mu(\mu^{2}-1)\\
	\frac{\partial\mu}{\partial\ln H(z)} & =-\mu(\mu^{2}-1)\\
	\frac{\partial k}{\partial\ln D_{A}(z)} & =k(\mu^{2}-1)\\
	\frac{\partial k}{\partial\ln H(z)} & =k\mu^{2}
	\end{align}
	
	
	\label{eq: derivs-of-k-mu-AP}\end{subequations}

The interesting thing about \cref{eq: derivs-of-k-mu-AP} is that
one can see that when evaluating the derivative of $P_{\rm obs}$ w.r.t
a cosmological parameter, the scales $k$ and the direction cosine
angles $\mu$ get mixed, giving a very powerful probe of cosmology.


\subsection{Fisher matrix operations}

Another important part of the code is the handling of the Fisher matrix itself for post-analysis.
We are interested in the constraints on the model parameters and therefore we need to calculate
the covariance matrix $\mathbf{C} = F^{-1}$, whose diagonal contains the square of the fully marginalized 1$\sigma$
errors of the model parameters and its off-diagonal entries encode the level of correlation among parameters.

\textbf{Maximization: }
If we have a Fisher matrix corresponding to an $N$ number of parameters, and we want to \textit{maximize} over the parameter $\theta_m$, i.e. we want to obtain the errors on all other parameters,
when the parameter $\theta_m$ is fixed, then we just need to remove from the Fisher matrix, the rows and the columns
corresponding to that parameter at position $m$:
\beeq$
\tilde F_{(N-1) \times (N-1)} = M_{(N-1) \times N}^{T} F_{N \times N}^{\,} M_{N \times (N-1)}^{\,} 
$
where the matrix $M$ can be defined in terms of the Kronecker delta $\delta_{i,j}$ as:
\beeqc$ \label{eq:fix-matrix}
M_{i j} = \delta_{\mho(i) , j}
$
where:
\beeqc$ \label{eq:mho-piecewise}
\mho(i) \equiv
\begin{cases} 
i ,& i < m \\
N ,& i = m \\
i-1 ,& i > m 
\end{cases}
$
and the indices run as $i=\{1,\ldots,N\}$ and $j=\{1,\ldots,(N-1)\}$.
The new Fisher matrix $\tilde F$ will be then 
of dimensions $(N-1) \times (N-1)$.

\textbf{Marginalization: }
If we want to \emph{marginalize} over the parameter $m$, then we have to invert the matrix $F$ to obtain the parameter covariance matrix $C=F^{-1}$ and remove from $C$
the columns and rows corresponding to the index $m$:
\beeqc$
\tilde C_{(N-1) \times (N-1)} = M_{(N-1) \times N}^{T} C_{N \times N}^{\,} M_{N \times (N-1)}^{\,} 
$
where $M$ is given by \cref{eq:fix-matrix} and \cref{eq:mho-piecewise}.
The operation of multiplying by matrix $M$ and its transpose can be done more efficiently for a large number of parameters to be removed, 
by using appropriate functions in the $\texttt{Wolfram Mathematica}$ language.

\textbf{Combination:} 
If we want to combine the Fisher matrix from an observation (or experiment) $A$
with another Fisher matrix from experiment $B$, where both $F_A$ and $F_B$ have
the same fiducial parameters and the same ordering, we just need to add the Fisher matrices together: $F_{A+B} = F_A + F_B$ and from $F_{A+B}$ we can obtain the combined contours and errors.

\textbf{Changing of parameter basis: }
If our Fisher matrix $F$ was performed in the parameter basis $X^i$ and we want to change to a different parameter basis $Y^i$; for example
in $\lcdm$ when we want to go from $\{\Omega_{m}, \Omega_{b}, h, \sigma_{8}  \}$ to  $\{\omega_{c}, \omega_{b}, h, \ln(10^10 A_s)  \}$,
we can use a simple Jacobian operation. This can be done since, as we already mentioned, the Fisher matrix is composed of derivatives 
of the likelihood, which in turn are in our case first derivatives of the data covariance matrix $\mathbf{C}$:
\beeq$
F_{\alpha \beta} \propto \parder{\mathbf{C}}{X^{\alpha}} \parder{\mathbf{C}}{X^{\beta}}
$
Therefore we can write the transformed Fisher matrix 
$\tilde F$ in the new basis as:
\beeqal$
\tilde{F}_{\mu \nu} & \propto \parder{X^{\alpha}}{Y^{\mu}} \parder{\mathbf{C}}{X^{\alpha}} \parder{\mathbf{C}}{X^{\beta}} \parder{X^{\beta}}{Y^{\nu}} \nonumber\\
\tilde{F}_{\mu \nu} & = J^T F J
$
with $J_{\alpha \nu} = \partial X^{\alpha}/ \partial Y^{\nu} $.
In some cases, if the "old" parameter basis $X^i$ cannot be solved in terms of the "new" parameter basis $Y^i$,
then one can use the identity:
\beeqp$
J_{a b}^{-1} =  \parder{Y^{a}}{X^{b}}
$

\textbf{Ellipsoidal confidence contour regions: }
Since we are assuming a Gaussian likelihood at the maximum, the confidence contours for a 2-dimensional slice of the likelihood
will be ellipsoidal, simply from the fact that the exponent of the two-dimensional Gaussian probability distribution for a set of two parameters $X_i = \{ x_1, x_2 \}$ and a symmetric covariance matrix $C_{i j} \equiv c_{i j} = c_{j i} $ contains the term:
\beeqp$ \label{eq:bivariate-exponent}
X_i C^{-1}_{i j} X_j = \left(  \frac{ c_{22} x_1^2}{\det{C}} +  \frac{ c_{11} x_2^2}{\det{C}}  - \frac{ 2 c_{12} x_1 x_2}{\det{C}}   \right)
$
Then the ellipse will be oriented along the eigenvectors of $C_{ij}$ with an anlge $\alpha$ with respect to the coordinate axes:
\beeq$
\tan ( 2 \alpha )  = \frac{2 c_{12}}{c_{11}^2 - c_{22}^2}
$
By integrating a bi-variate Gaussian \cref{eq:moreGeneral-Gaussian-Likelihood} with the exponent \cref{eq:bivariate-exponent} to find
the confidence regions as in \cref{eq:likeli-confidregions}, we obtain that the 1, 2 and 3$\sigma$ confidence regions
with major semi-axes $a$ and minor semi-axes $b$ are given by:
\beeqal$
1\sigma: & \quad a = 1.51 \sqrt{\lambda_1};\quad b=1.51\sqrt{\lambda_2} \nonumber \\
2\sigma: & \quad a = 2.49 \sqrt{\lambda_1};\quad b=2.49\sqrt{\lambda_2} \nonumber \\
3\sigma: & \quad a = 3.44 \sqrt{\lambda_1};\quad b=3.44\sqrt{\lambda_2} \quad , \nonumber
$
where $\lambda_1,\; \lambda_2$ are the largest and the smallest eigenvalues, respectively.
The $2 \times 2$ matrix from where this ellipses can be visualized can be either obtained 
by marginalizing or maximizing over all the other $(N-2)$ parameters. 
The \fito code contains these options and many other tools for plotting and visualization.

% \subsection{Validation of the code within the Euclid collaboration}
% \begin{itemize}
% \item 4 Steps: from the basics to the more complicated
% \item Shape parameters, z-obswevables and non-diagonal terms
% \item WL: Cases
% \end{itemize}

\subsection{Extensions of the Fisher matrix approach}

The Fisher matrix a



%----------------------------------------------------------------------------------------
